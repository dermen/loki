import numpy as np
import h5py
import json
from RingData import InterpSimple, DiffCorr

from pylab import show,imshow

def generate_sacla_images( runfile , runnumber, num_dark=0):
    """
    runfile,   str, filename of the run hdf5
    runnumber,  str, run number (e.g. '178884')
    num_dark,    int, number of dark imgs in h5 file, should only be at the start of the file
    """

    fh5           = h5py.File( runfile )
    imgs_path     = '/run_%s/detector_2d_assembled_1'%runnumber
    exposure_tags =  fh5[ imgs_path].keys()[1+num_dark:] # fist key not a tag, then there might be some darks if its the start of the run
    
    img_gen       = (  fh5[ imgs_path + '/' + tag + '/detector_data' ].value for tag in exposure_tags )
    return img_gen, exposure_tags


def normalize_polar_images( imgs, mask_val = -1 ): 
    norms = np.ma.masked_equal( imgs, mask_val).mean(axis=2)
    imgs /= norms[:,:,None]
    imgs[ imgs < 0 ] = mask_val
    return imgs


def interpolate_run ( run_number, img_gen, tags, mask, x_center, y_center, qmin, qmax, pixsize, detdist, wavelen, nphi, prefix=None):
    
    """
    run_number,  string rung number
    img_gen,     generator of 2d np.array float images, this should generate each image in a run
    tags,        list, string,  the tags associated w each image generated by img_gen, (should be in order with generation)
    mask,        2d np.array bool, a masked image, True is unmasked, False is masked, should have same dimensions of images generated by img_gen
    x_center,    float, pixel unit where beam hits detector, x dimension( fast dimension), measured from 0,0 pixel corner
    y_center,    float,  pixel unit where beam hits detector, y dimension( slow dimension), measured from 0,0 pixel corner
    qmin, qmax,  float, min q and max q bounds of each polar image being created (in inverse angstroms)
    pixsize,     float pixel size in meter
    detdist,     float, sample to detector distance in meter
    wavelen,     float, wavelength of photons in angstroms
    nphi,        int,  azimuthal dimension of polar image, (try to keep at least single pixel resolution at qmax, you can average over polar pixels later) 
    prefix       str, file prefix, this script makes two files, (this parameter will be the prefix of each)
    """

    num_imgs = len(tags)

    if not prefix:
        prefix = run_number
    output_hdf = h5py.File(  prefix + '.hdf5', 'w' )

#   some useful functions
    pix2invang  = lambda qpix : np.sin(np.arctan(qpix*pixsize/detdist )/2)*4*np.pi/wavelen 
    invang2pix  = lambda qia  : np.tan(2*np.arcsin(qia*wavelen/4/np.pi))*detdist/pixsize

    qmin_pix = invang2pix ( qmin )
    qmax_pix = invang2pix ( qmax )

#   Initialize the interpolater
    interpolater  = InterpSimple( x_c, y_c, qmax_pix, qmin_pix, nphi, raw_img_shape = mask.shape )

#   make a polar image mask
    pmask   = interpolater.nearest( mask , dtype=bool ).round()

#   Make the polar images
    polar_imgs = np.array( [ pmask * interpolater.nearest( img_gen.next() ) for dummie in range( num_imgs) ] )

#   save the data raw
    output_hdf.create_dataset( 'polar_data',data = polar_imgs)

#   Consider normalizing the images 
    polar_imgs = normalize_polar_images( polar_imgs) 

#   save the data normalized
    output_hdf.create_dataset( 'normalized_polar_data',data = polar_imgs)

#   save a lookup-map
    tag_map = {}
    for indx,tag in enumerate( tags ):
        tag_map[tag] = indx

#   save meta data
    output_hdf.create_dataset( 'polar_mask',data = pmask.astype(int))
    output_hdf.create_dataset( 'x_center',   data = x_center)
    output_hdf.create_dataset( 'y_center',   data = y_center)
    output_hdf.create_dataset( 'num_phi',   data = nphi)
    output_hdf.create_dataset( 'wavelen' ,   data = wavelen)
    output_hdf.create_dataset( 'pixsize' ,     data = pixsize)
    output_hdf.create_dataset( 'detdist' ,     data = detdist)

#   save the q-mapping
    qrange_pix = np.arange( qmin_pix, qmax_pix )
    q_map       =  np.array( [ [ ind, pix2invang(q) ] for ind,q in enumerate( qrange_pix) ] )
    output_hdf.create_dataset( 'q_mapping' ,    data = q_map )

#   save
    output_hdf.close()

#   save the lookup-map
    dump_file = open( prefix + '.json', 'w')
    json.dump( tag_map, dump_file )
    dump_file.close()

    return prefix + '.hdf5', prefix + '.json' # return the file names it created


def correlate_polar_images( data_hdf_fname, tag_map_fname, tag_pairs_fname ):

    """
    data_hdf_fname,     string, hdf5 file name, hdf data file returned by interpolate_run 
    tag_map_fname,      sting, JSON file name,  tag map file returned by interpolate_run
    tag_pairs_fname,    string, JSON file name, contains a list of exposure tag pairs,
                                                and each pair will be loaded, and subtracted
    """

#   load the polar data
    data_hdf    = h5py.File( data_hdf_fname )

    polar_data = data_hdf['normalized_polar_data']
    q_map     = { i: val for i,val in data_hdf[ 'q_mapping'].value }
    nphi    = data_hdf['num_phi'].value
    nq      = len( q_map)


###############################
#                             #
#   PROCESS POLAR DATA HERE   #
#                             # 
###############################


#   load out map
    tag_map = json.load( open( tag_map_fname) )

#   load the exposure tag pairs
    tag_pairs = json.load( open( tag_pairs_fname ))

    exposure_diffs = [] 
    for i_, tags in enumerate( tag_pairs):
        tagA, tagB = tags
        try: indxA = tag_map[ tagA]
        except KeyError: continue
        try: indxB = tag_map[ tagB]
        except KeyError: continue
        shotA = polar_data[ indxA ]
        shotB  = polar_data[ indxB]
        exposure_diffs.append(    shotA - shotB  )
    exposure_diffs = np.vstack( exosure_diffs ) 

#   take the autocorrelation of each pair
    DC     = DiffCorr( exposure_diffs ) 
    cor      = DC.autocorr()
#################################
#                               #
#   PROCESS CORRELATIONS HERE   #
#                               # 
#################################

#   take the mean over pairs
    cor_m = cor.mean(0)


# USAGE
##################
# RUN PARAMETERS # 
##################
x_c, y_c     = 1199.0093935 ,  1197.28156522 # latest optimzation in pixel units
qmin         = 2.1         # inverse angstroms
qmax         =  2.9        # inverse angstrom
pixsize      = 0.00005     # meter
wavelen      = 1.44        #angstrom
detdist       = 0.053      #meter
nphi          = 5000       # azimuthal dimension of interpolated image (careful not to downsample, the code is not very smart!, keep at least single pixel precision at qmax) 
run_number    = '178884'
mask         = np.load( '/Users/mender/for_schengjun/mask.npy' )  

img_gen, tags           = generate_sacla_images ( 'subset_178884.h5', run_number, num_dark=50 ) # only for this h5 file, not all h5 files will have these darks
data_file, tag_map_file = interpolate_run( run_number, img_gen, tags,  mask, x_c, y_c, qmin, qmax, pixsize, detdist, wavelen, nphi )

#####################################################
# REPLACE THIS STEP      

# generate simple tag par list for this run
tag_list_file = '%s_tag_pairs.json'%run_number
# simply correlated every other exposure, except for the first 50 exposures as they are background
simple_map = zip( tags[50:-1], tags[51:] )
dump_f = open( tag_list_file , 'w')
json.dump( simple_map, dump_f )
dump_f.close()  

######################################################

correlate_polar_images( data_file, tag_map_file,tag_list_file)



